
# Vegetable Classification - Training Notebook
# Upload this notebook to Google Colab to train your model.

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import numpy as np
import os
import zipfile

# --- 1. Upload Dataset ---
# Instructions:
# 1. Zip your 'Vegetable Images' folder on your desktop.
# 2. Upload the 'Vegetable Images.zip' file to the Colab runtime (left sidebar).

# Determine if running in Colab or locally with manual intervention
# For Colab, we assume /content/Vegetable Images after unzip
base_dir = '/content/Vegetable Images' 

# Uncomment the following block if you are uploading a zip file in Colab
# with zipfile.ZipFile("/content/Vegetable Images.zip", 'r') as zip_ref:
#     zip_ref.extractall("/content/")

train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'test')

# --- 2. Data Preprocessing ---
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

test_datagen = ImageDataGenerator(rescale=1./255)

# Assuming 150x150 image size as per model summary
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150,
150),
    batch_size=32,
    class_mode='categorical'
)

validation_generator = test_datagen.flow_from_directory(
    validation_dir,
    target_size=(150,
150),
    batch_size=32,
    class_mode='categorical'
)

# Test Generator for Evaluation
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(150,
150),
    batch_size=32,
    class_mode='categorical'
)

# Print the class encodings done by the generators
class_map = dict([(v, k) for k, v in train_generator.class_indices.items()
])
# Invert class map for prediction decoding (index -> name)
class_map_inv = {v: k for k, v in class_map.items()
}
print("Class Map:", class_map)

# --- 3. Model Building ---
model = Sequential()

# Convolutional layer with 32 filters, kernel size 3x3, 'same' padding, ReLU activation
model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=[
    150,
    150,
    3
]))
model.add(MaxPooling2D(pool_size=(2,
2)))

# Convolutional layer with 64 filters, kernel size 3x3, 'same' padding, ReLU activation
model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=(2,
2)))

model.add(Flatten())

# Fully connected layer with 128 units and ReLU activation
model.add(Dense(128, activation='relu'))

# Dropout layer with 0.25 rate
model.add(Dropout(0.25))

# Fully connected layer with 128 units and ReLU activation
model.add(Dense(128, activation='relu'))

# Output layer with 15 units and softmax activation
model.add(Dense(15, activation='softmax'))

# print the model summary
model.summary()

# --- 4. Compile Model ---
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'
])

# --- 5. Train Model ---
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // 32,
    epochs=100,
    verbose=1,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // 32,
    callbacks=[early_stopping
]
)

# --- 6. Plotting Results ---
h = history.history
plt.style.use('ggplot')
plt.figure(figsize=(10,
5))
plt.plot(h['loss'
], c='red', label='Training Loss')
plt.plot(h['val_loss'
], c='red', linestyle='--', label='Validation Loss')
plt.plot(h['accuracy'
], c='blue', label='Training Accuracy')
plt.plot(h['val_accuracy'
], c='blue', linestyle='--', label='Validation Accuracy')
plt.xlabel("Number of Epochs")
plt.legend(loc='best')
plt.show()

# --- 7. Evaluation & Save ---
print("Evaluating on Test Set...")
model.evaluate(test_generator)

model.save('vegetable_classification.h5')
print("Model saved as vegetable_classification.h5")

# --- 8. Specific Image Prediction Test ---
def generate_predictions(test_image_path, actual_label):
    
    # Check if file exists first
    if not os.path.exists(test_image_path):
        print(f"File not found: {test_image_path}")
        return

    # 1. Load and preprocess the image
    img = tf.keras.utils.load_img(test_image_path, target_size=(150,
150))
    img_array = tf.keras.utils.img_to_array(img)
    img_array_normalized = img_array / 255.0
    img_input = np.expand_dims(img_array_normalized, axis=0)

    # 2. Make Predictions
    predictions = model.predict(img_input)
    predicted_label_index = np.argmax(predictions)
    predicted_vegetable = class_map_inv[predicted_label_index
]
    
    plt.figure(figsize=(4,
4))
    plt.imshow(img_array_normalized)
    plt.title("Predicted Label: {}, Actual Label: {}".format(predicted_vegetable, actual_label))
    plt.grid(False)
    plt.axis('off')
    plt.show()

# Test with a specific image (adjust path if needed)
# Example usage:
# generate_predictions('/content/Vegetable Images/test/Broccoli/1011.jpg', 'Broccoli')

# Try to find a random image to test automatically
import random
if os.path.exists(test_dir):
    classes = [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))
]
    if classes:
        random_class = random.choice(classes)
        class_path = os.path.join(test_dir, random_class)
        images = os.listdir(class_path)
        if images:
            random_image = random.choice(images)
            full_path = os.path.join(class_path, random_image)
            print(f"Testing with random image: {full_path}")
            generate_predictions(full_path, actual_label=random_class)
